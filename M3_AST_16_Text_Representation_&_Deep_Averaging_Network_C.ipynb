{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-16/blob/main/M3_AST_16_Text_Representation_%26_Deep_Averaging_Network_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment : Natural Language Processing - I (Text representation and Deep Averaging Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSEE2MI6FWZ1"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        " \n",
        "*  generate vector representation of words in the data using Glove embeddings\n",
        "*  find the cosine similarity between the words present in the data\n",
        "*  plot the word vector representation using TSNE\n",
        "*  implement Deep Averaging Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_16_Text_Representation_&_Deep_Averaging_Network_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\")\n",
        "    ipython.magic(\"sx unzip glove.6B.zip\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/rt-polarity.zip\")\n",
        "    ipython.magic(\"sx unzip rt-polarity.zip\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8amkxNZMOBE"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The **sentence polarity dataset v1.0** contains two data files which are: \n",
        "  * **rt-polarity.pos**: It contains 5331 positive examples\n",
        "  * **rt-polarity.neg**: It contains 5331 negative examples\n",
        "\n",
        "Each line in these two files corresponds to a single snippet (usually\n",
        "containing roughly one single sentence) that includes the review of a movie. \n",
        "\n",
        "**Note:** Here is the source [link](https://www.cs.cornell.edu/people/pabo/movie-review-data/) to the Movie  dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C-eklopT0q0"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The aim of this assignment is to study the use of pre-trained word embeddings using GloVe for text representation. The pre-trained word embeddings of different dimensions are provided. A simple way to represent a piece of text is to tokenize, vectorize and average the word vector representations of all the words in\n",
        "the text. You will study the use of this simple approach of text representation.\n",
        "\n",
        "1. We will use t-SNE to plot the word vector representations.\n",
        "Indicate different classes with different colors. Also, we have used 50-dimensional word embeddings in this experiment. *If you want to study the use of different dimensional word (100d,200,or 300d), just change the dimensions and use the embeddings.*\n",
        "2. Design a classifier using Deep Averaging Network (DAN) on datasets provided and report the test set accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OezYgKMk11WU"
      },
      "source": [
        "### Importing the libraries and packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_QuoG7ZLK_t"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "import matplotlib.patches as mpatches\n",
        "tsne = TSNE(n_components=2)\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8nF44dZ14eG"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the positive and negative files and split the sentences into a list\n",
        "with open('rt-polarity.neg',\"r\") as data_neg:\n",
        "  data_neg_set = data_neg.read().splitlines()\n",
        "\n",
        "with open('rt-polarity.pos',\"r\") as data_pos:\n",
        "  data_pos_set = data_pos.read().splitlines()"
      ],
      "metadata": {
        "id": "_fzYuJSqCPGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the positive and negative reviews\n",
        "len(data_neg_set), len(data_pos_set)"
      ],
      "metadata": {
        "id": "CM-TmdP0HG4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the negative reviews\n",
        "data_neg_set = pd.DataFrame(data_neg_set, columns=[\"Review\"])\n",
        "\n",
        "# Loading the positive reviews\n",
        "data_pos_set = pd.DataFrame(data_pos_set, columns=[\"Review\"])"
      ],
      "metadata": {
        "id": "P0nQbQmOI16K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first five rows of the positive examples\n",
        "data_pos_set.head()"
      ],
      "metadata": {
        "id": "hQj1GNFxI-7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first five rows of the negative examples\n",
        "data_neg_set.head()"
      ],
      "metadata": {
        "id": "9y4sQ5KUWvgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJh13HlV16WC"
      },
      "source": [
        "#### Giving the labels to the data\n",
        "\n",
        "Let us give the labels as positive and negative for the sentences present in the two files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G95kzywrfCIJ"
      },
      "source": [
        "data_neg_set['Label'] = 'Negative'\n",
        "data_pos_set['Label'] = 'Positive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tawtWMsWOzzU"
      },
      "source": [
        "Let us have a glance at few of the values present in the data with negative and positive reviews that we have labeled in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_RweIcy-Elq"
      },
      "source": [
        "data_neg_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dz9sNPz_qWD"
      },
      "source": [
        "data_pos_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fftKy58R2BQ7"
      },
      "source": [
        "#### Combining the positive and negative data\n",
        "\n",
        "Now, we have to work on the combined data containing the positive and negative reviews, so, let us concatenate both the dataframes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNpIJTwfoju"
      },
      "source": [
        "dataframes = [data_neg_set, data_pos_set]\n",
        "rt_polarity_data = pd.concat(dataframes)\n",
        "rt_polarity_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fErTZ4xY_ag0"
      },
      "source": [
        "\n",
        "From above we can see that due to concatenation the last row index is 5330 that we can see below but that should be 10661."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI1Q71Gsi0m6"
      },
      "source": [
        "Therefore, we will reset the index so that last row has index of 10661."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9YGORtVeO-L"
      },
      "source": [
        "rt_polarity_data.reset_index(drop = True, inplace = True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FziLZY2InQsl"
      },
      "source": [
        "rt_polarity_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXeLwOabUfRI"
      },
      "source": [
        "When you combine the negative and positive examples, it is a good idea to shuffle the examples so that the negative and positive examples are spread throughout. If we do not shuffle it, then, it may happen that in some mini-batches, examples from only one class(positive or negative) will be present. Therefore, it is better to avoid such scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLlRyfX3QIeu"
      },
      "source": [
        "rt_polarity_data = shuffle(rt_polarity_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU9rwn1tVWMT"
      },
      "source": [
        "Now, let us look at the shuffled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gn1zAGNQQaa"
      },
      "source": [
        "rt_polarity_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrVrS5SmP2-z"
      },
      "source": [
        "Let us check the value counts of negative and positive reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiXfMlw8iWct"
      },
      "source": [
        "rt_polarity_data['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK3WHmukP-Ya"
      },
      "source": [
        "Checking whether there are any null values present in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGk4tTa3bcrj"
      },
      "source": [
        "rt_polarity_data.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtF1BH1sQG_n"
      },
      "source": [
        "From above we can see that there are no null values present in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMIU5YtCv0k8"
      },
      "source": [
        "### Data Preprocessing \n",
        "\n",
        "In this step, we will preprocess the data to remove non-alphabet (punctuations and numbers), stop words,  and lower case all of the reviews present in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8J-xAfCEQej"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    \n",
        "    sen = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sen)     \n",
        "    sen = re.sub(r\"\\'s\", \" \\'s\", sen) \n",
        "    sen = re.sub(r\"[\\([{})\\]]\", \"\", sen)\n",
        "\n",
        "    # Tokenizing words\n",
        "    tokens = word_tokenize(sen)  \n",
        "\n",
        "    # Converting to lower case\n",
        "    tokens = [w.lower() for w in tokens]    \n",
        "\n",
        "     # Remove punctuations\n",
        "    table = str.maketrans('', '', string.punctuation) \n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    # Remove non alphabet\n",
        "    words = [word for word in stripped if word.isalpha()]  \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove stop words\n",
        "    words = [w for w in words if not w in stop_words]  \n",
        "    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjzvZSmwvO5O"
      },
      "source": [
        "# Store the preprocessed reviews in a new list\n",
        "lines_pos = []\n",
        "sentences_pos = list(data_pos_set['Review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G18YceU7mgAx"
      },
      "source": [
        "for sen in sentences_pos:\n",
        "    # Call the preprocess_text function on each sentence of the review text \n",
        "    lines_pos.append(preprocess_text(sen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqkMzMjrAe4V"
      },
      "source": [
        "# Check for the length of the preprocessed text\n",
        "len(lines_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viX5aijFhlGf"
      },
      "source": [
        "# Print the preprocessed text for the first review\n",
        "print(lines_pos[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im3f7SLUjV1t"
      },
      "source": [
        "Further, let us perform the data preprocessing for negative reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HYOmrbmja16"
      },
      "source": [
        "# Store the preprocessed reviews in a new list\n",
        "lines_neg = []\n",
        "sentences_neg = list(data_neg_set['Review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o23kCfKja16"
      },
      "source": [
        "for sen in sentences_neg:\n",
        "  # Call the preprocess_text function on each sentence of the review text \n",
        "  lines_neg.append(preprocess_text(sen))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mohBwmmjja17"
      },
      "source": [
        "# Check for the length of the preprocessed text\n",
        "len(lines_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B2PK5B8ja18"
      },
      "source": [
        "# Print the preprocessed text for the first review\n",
        "print(lines_neg[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfVCNuhgUgE7"
      },
      "source": [
        "Now, as we can see that lines_neg and lines_pos are the lists of list. So, we have to convert it into a simple list so that we can use the elements(words) of the list to generate vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVi2EqRreMml"
      },
      "source": [
        "text_neg = list(itertools.chain.from_iterable(lines_neg))\n",
        "text_pos = list(itertools.chain.from_iterable(lines_pos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy518l5_bmtq"
      },
      "source": [
        "Let us look into the first 10 values in text_neg and text_pos and the length of both the lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn8IJ0U7c8gM"
      },
      "source": [
        "text_neg[0:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuOyBEK5brGZ"
      },
      "source": [
        "text_pos[0:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odNlF-4PdFeR"
      },
      "source": [
        "len(text_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MI8ppdjbzia"
      },
      "source": [
        "There are 55688 words for negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHOj-gIzJqZi"
      },
      "source": [
        "len(text_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SUBk3OFb75M"
      },
      "source": [
        "There are 56832 words for positive reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDwNJHcfPaKd"
      },
      "source": [
        "### **Load the GloVe embeddings**\n",
        "\n",
        "**What is GloVe?**\n",
        "\n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. These are essential for solving most Natural language processing problems.The resulting embeddings show interesting linear substructures of the word in vector space.\n",
        "\n",
        "Thus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.\n",
        "\n",
        "Now, let us load the 50-dimensional GloVe embeddings. But if you want to use 100d, 200d, or 300d embeddings, then, just change the dimensions in the name of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZt0USXAZVnk"
      },
      "source": [
        "GloVe_Dict_50d = {}\n",
        "# Loading the 50-dimensional vector of the model\n",
        "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      GloVe_Dict_50d[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9q0G-ApcVL7"
      },
      "source": [
        "Let us have a look at the Glove_Dict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ONBhVRYz0I-"
      },
      "source": [
        "GloVe_Dict_50d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thdIOpEgjGhz"
      },
      "source": [
        "Above, we can see that each word is represented by 50 values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEswIOf9xeAg"
      },
      "source": [
        "# Length of the word vocabulary\n",
        "print(len(GloVe_Dict_50d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mL3m1Kcgi0"
      },
      "source": [
        "From above, we can see that the glove dictionary consist of 400000 words. Further, let us create vector representation of the words for negative and positive reviews using the glove dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loEMDCGRgiax"
      },
      "source": [
        "#### Vector representation of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eBRiHNB-WJ1"
      },
      "source": [
        "def gen_vectors(text):\n",
        "  vectors = []\n",
        "  for word in text:\n",
        "    try:\n",
        "      vector = GloVe_Dict_50d[word]\n",
        "      vectors.append(vector)\n",
        "    except KeyError:\n",
        "      pass\n",
        "  print(\"There are %d words and the vector size of each word is %d\" %((len(vectors),len(vectors[0]))))\n",
        "  return vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgtsH64CK0Tz"
      },
      "source": [
        "# Passing the words present in text_neg and text_pos to the function gen_vectors\n",
        "vectors_neg = gen_vectors(text_neg)\n",
        "vectors_pos = gen_vectors(text_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmXvUFOt9fJv"
      },
      "source": [
        "\n",
        "### Find the similarity between words \n",
        "\n",
        "Here, we will measure the similarity between the words using cosine_similarity. Now, let us first understand about **cosine similarity**.\n",
        "\n",
        "**cosine similarity** : Cosine similarity is one of the metric to measure the text-similarity between two documents irrespective of their size in Natural language Processing. If the Cosine similarity score is 1, it means two vectors have the same orientation and they are closely similar. The value closer to 0 indicates that the two words have less similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhu5139roGrX"
      },
      "source": [
        "First, we will find the cosine similarity between the words present in the negative reviews(text_neg)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuRZ_OZo0Z-y"
      },
      "source": [
        "# Creating an empty list for storing values of cosine similarity\n",
        "word_similarity = []\n",
        "index = []\n",
        "for i, word_1 in enumerate(text_neg):\n",
        "  row_wise_simiarity = []\n",
        "  print(i,word_1)\n",
        "  if(i==4):\n",
        "    break\n",
        "  for j, word_2 in enumerate(text_neg):\n",
        "    # Get the vectors of the word using GloVe\n",
        "    try:\n",
        "      vec_1, vec_2 = GloVe_Dict_50d[word_1], GloVe_Dict_50d[word_2]\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "    # As the vectors are in one dimensional, convert it to 2D by reshaping\n",
        "    vec_1, vec_2 = np.array(vec_1).reshape(1,-1), np.array(vec_2).reshape(1,-1) \n",
        "\n",
        "    # Measure the cosine similarity between the vectors.\n",
        "    similarity = cosine_similarity(vec_1, vec_2)\n",
        "    row_wise_simiarity.append(np.array(similarity).item())\n",
        "\n",
        "  # Store the cosine similarity values in a list  \n",
        "  word_similarity.append(row_wise_simiarity)\n",
        "  index.append(word_1)\n",
        "\n",
        "# Create a DataFrame to view the similarity between words\n",
        "pd.DataFrame(word_similarity, columns=text_neg,index = index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhqU87IYjj83"
      },
      "source": [
        "From the above similarity scores, we can observe that words which are closely similar have values close to 1 and which are non-similar are close to 0. For example, **simplistic** and **silly** has score 0.68, that means they can be considered as 68% similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN0DOdo3oXhY"
      },
      "source": [
        "Now, we will find the cosine similarity between the words present in the positive reviews(text_pos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce5UOpMj4EGD"
      },
      "source": [
        "word_similarity = []\n",
        "index = []\n",
        "for i, word_1 in enumerate(text_pos):\n",
        "  row_wise_simiarity = []\n",
        "  print(i,word_1)\n",
        "  if(i == 4):\n",
        "    break\n",
        "  for j, word_2 in enumerate(text_pos):\n",
        "    # Get the vectors of the word using GloVe\n",
        "    try:\n",
        "      vec_1, vec_2 = GloVe_Dict_50d[word_1], GloVe_Dict_50d[word_2]\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "    # As the vectors are in one dimensional, convert it to 2D by reshaping\n",
        "    vec_1, vec_2 = np.array(vec_1).reshape(1,-1), np.array(vec_2).reshape(1,-1) \n",
        "\n",
        "    # Measure the cosine similarity between the vectors.\n",
        "    similarity = cosine_similarity(vec_1, vec_2)\n",
        "    row_wise_simiarity.append(np.array(similarity).item())\n",
        "\n",
        "  # Store the cosine similarity values in a list  \n",
        "  word_similarity.append(row_wise_simiarity)\n",
        "  index.append(word_1)\n",
        "\n",
        "# Create a DataFrame to view the similarity between words\n",
        "pd.DataFrame(word_similarity, columns=text_pos, index = index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmE8_X8F3Czn"
      },
      "source": [
        "Again, from the above scores we can observe the similarity of words present in the list of words in positive reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YNNJh7QIwFu"
      },
      "source": [
        "### Visualization of word vectors using TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xH-RYYVYz-5"
      },
      "source": [
        "To represent the word vectors we will vectorize the **\"Review\"** column of **rt_polarity_data** and get the average of the vectors using the 50-dimensional GloVe embeddings.\n",
        "\n",
        "**How t-SNE works?**\n",
        "\n",
        "The intuition of what and how t-SNE works.\n",
        "\n",
        "Suppose you have a 50-dimensional data set, as it is like an impossible task for us to visualize and get a sense of it. We have to convert that 50D data set to something which we can visualize or with which we can play around. This is where t-SNE comes into the picture it converts the higher dimensional data into the lower dimensional data by following steps:-\n",
        "\n",
        "1. It measures the similarity between the two data points and it does for every pair. Similar data points will have more value of similarity and the different data points will have less value.\n",
        "\n",
        "2. Then it converts that similarity distance to probability(joint probability) according to the normal distribution.\n",
        "\n",
        "3. It does the similarity check for every point. Thus it will have the similarity matrix `S1` for every point. This is all calculation it does for our data points that lie in higher-dimensional space.\n",
        "\n",
        "4. Now, t-SNE arranges all of the data points randomly on the required lower-dimensional.\n",
        "\n",
        "5. And it does all of the same calculation for lower dimensional data points as it does for higher ones — calculating similarity distance but with a major difference it assigns probability according to t- distribution instead of normal distribution and this is because it is called t-SNE not simple SNE.\n",
        "\n",
        "6. Now we also have the similarity matrix for lower dimensional data points. Let’s call it S2.\n",
        "\n",
        "7. Now, what t-SNE does is it compares matrix S1 and S2 and tries to make the difference between matrix S1 and S2 much smaller by doing some complex mathematics.\n",
        "\n",
        "In the end, we will have lower-dimensional data points that try to capture even complex relationships at which PCA fails.\n",
        "So on a very high level, this is how t-SNE works.\n",
        "\n",
        "For this, we will define a function that will help us to get the glove embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8ckT4682Be"
      },
      "source": [
        "def glove_embeddings(text, dim):\n",
        "    if len(text) < 1:\n",
        "        return np.zeros(dim)\n",
        "    else:\n",
        "        vectorized = [GloVe_Dict_50d[word] if word in GloVe_Dict_50d else np.random.randn(dim) for word in text]  \n",
        "    sum = np.sum(vectorized, axis=0)\n",
        "    # Return the average vector\n",
        "    return sum/len(vectorized)     \n",
        "\n",
        "def get_glove_embeddings(text, dimension):\n",
        "        embeddings = text.apply(lambda x: glove_embeddings(x, dimension))\n",
        "        return list(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON4GX8EK82CA"
      },
      "source": [
        "Getting the embeddings for 'Review' column of the rt-polarity data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79FAOtMn82CB"
      },
      "source": [
        "word_embeddings = get_glove_embeddings(rt_polarity_data['Review'], dimension=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0mAIiGY62P-"
      },
      "source": [
        "Now, let us visualize the positive and negative reviews associated with the word embeddings.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmvskt-DpjyN"
      },
      "source": [
        "def tsne_visualization(word_embeddings):\n",
        "    x = word_embeddings[1:100]\n",
        "    x = np.asarray(x)\n",
        "    y = tsne.fit_transform(x)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    colors=['orange','red']\n",
        "    sns.scatterplot(x=y[:,0],y=y[:,1],hue=rt_polarity_data['Label'].iloc[1:100])\n",
        "\n",
        "    for label,x,y in zip(rt_polarity_data['Label'].iloc[1:100],y[:, 0],y[:,1]):\n",
        "        plt.annotate(label,xy=(x,y),xytext=(0,0),textcoords='offset points')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCUruRSuOTF8"
      },
      "source": [
        "tsne_visualization(word_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FcNbfLSrq3f"
      },
      "source": [
        "Further, let us move on to the next topic which is Deep Averaging Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBbfvfhCeO-G"
      },
      "source": [
        "### Deep Averaging Network\n",
        "\n",
        "Vector space models for natural language processing (NLP) represent words using low dimensional vectors called embeddings. To apply vector space\n",
        "models to sentences or documents, one must first select an appropriate composition function, which is a mathematical process for combining multiple\n",
        "words into a single vector. Composition functions fall into two classes: **unordered** and **syntactic**. **Unordered functions** treat input texts as bags of word embeddings, while **syntactic functions** take word order and sentence structure into account. \n",
        "\n",
        "Previously published experimental results have shown that syntactic functions outperform unordered functions on many tasks. However, there is a tradeoff: syntactic functions\n",
        "require more training time than unordered composition functions and are prohibitively expensive in\n",
        "the case of huge datasets or limited computing resources. \n",
        "\n",
        "Therefore, we introduce a deep unordered model that obtains near state-of-the-art accuracies on a variety of sentence and document-level tasks with just minutes of training time on an average laptop computer. This model, the deep averaging network (DAN), works in three simple steps:\n",
        "\n",
        "1. Take the vector average of the embeddings\n",
        "associated with an input sequence of tokens.\n",
        "2. Pass that average through one or more feedforward layers.\n",
        "3. Perform (linear) classification on the final\n",
        "layer’s representation.\n",
        "\n",
        "The model can be improved by applying a novel dropout-inspired regularizer: for each training instance, randomly drop some of the tokens’ embeddings before computing the average.\n",
        "\n",
        "Also, let us look into the architecture of Deep Averaging Network that we will create.\n",
        "\n",
        "![image.png](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/DNN.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHuf_CQ2Oek"
      },
      "source": [
        "In the above architecture, we pass the average of word vectors into the layers of neural network to train the model and perform the classification at the final layer.\n",
        "\n",
        "Now, let us do some preprocessing before craeting the Deep Averaging Network. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VykHHDNwE7zD"
      },
      "source": [
        "We can preprocess the text using gensim package. Gensim provides function **simple_preprocess** for more effective preprocessing of the corpus. In such kind of preprocessing, we can convert a document into a list of lowercase tokens. We can also ignore tokens that are too short or too long.\n",
        "\n",
        "**Note:** Refer to the following [link](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess) for gensim `simple_preprocess` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdlbVXuNXdAe"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "rt_polarity_data['Review'] = rt_polarity_data['Review'].apply(lambda x:simple_preprocess(x, max_len=40))\n",
        "\n",
        "# Remove stop words\n",
        "rt_polarity_data['Review'] = rt_polarity_data['Review'].apply(lambda x: [w for w in x if not w in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpMsVNXUeO-M"
      },
      "source": [
        "rt_polarity_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9iKYUClFkY3"
      },
      "source": [
        "Further, we will replace the positive label with '1' and negative with '0'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG3iBvH7HMUj"
      },
      "source": [
        "rt_polarity_data[\"Label\"] = rt_polarity_data[\"Label\"].apply(lambda x:1 if x == \"Positive\" else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1os5M46f_qB"
      },
      "source": [
        "rt_polarity_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVcPBFCGLVw"
      },
      "source": [
        "Storing the 'Review' and the 'Label' column in X and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct1xcFhxO3vV"
      },
      "source": [
        "X = rt_polarity_data[\"Review\"]\n",
        "y = rt_polarity_data['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfpfXB13KYj3"
      },
      "source": [
        "Getting the glove embeddings for 'Review' column of the rt-polarity data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FLhOo3cS3QG"
      },
      "source": [
        "train_embeddings  = get_glove_embeddings(rt_polarity_data['Review'], dimension=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD6fg7EpS3Nu"
      },
      "source": [
        "len(train_embeddings[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhR4rHURKj3l"
      },
      "source": [
        "Let us now prepare the data for train and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4d2qUBJYFkk"
      },
      "source": [
        "# Storing the train_embeddings in X\n",
        "X = np.array(train_embeddings)\n",
        "\n",
        "# Converting X into torch tensor\n",
        "X = torch.Tensor(X)\n",
        "\n",
        "# Reshaping X to 200 dimension\n",
        "X = X.reshape(-1, 50)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eS3UxZ8YBnz"
      },
      "source": [
        "# Storing the labels in y\n",
        "y = rt_polarity_data['Label']\n",
        "\n",
        "# Converting X into torch tensor\n",
        "y = torch.Tensor(y)\n",
        "\n",
        "# Reshaping y to 1 dimension\n",
        "y = y.reshape(-1,1)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcf2jdsnMGC4"
      },
      "source": [
        "Here, we will split the data into train and test part with test size as 0.15 equivalent to 15% of the data using stratified sampling. Moreover,  stratified sample is one that ensures that subgroups (strata) of a given population are each adequately represented within the whole sample population of a research study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmZbAI7gX1f9"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, train_size = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpYUqu_SyWC5"
      },
      "source": [
        "# Set up device to run CUDA operations\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhD1EGsRMxKP"
      },
      "source": [
        "Preparing the training data and testing data with batch size 64 using Tensor Dataset and Tensor Data Loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHF1dHQh8xSa"
      },
      "source": [
        "train_dataset = TensorDataset(X_train,y_train)\n",
        "train_loader = DataLoader(train_dataset,batch_size = 64)\n",
        "test_dataset = TensorDataset(X_test,y_test)\n",
        "test_loader = DataLoader(test_dataset,batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsdim8jsXvHb"
      },
      "source": [
        "#### Building the DAN model\n",
        "\n",
        "In this Deep Averaging Network model, we using 3 layers with dropout as 50% and batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksFLxqGOjHMT"
      },
      "source": [
        "class DAN(torch.nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, dp = 0.5, d_out = 2):\n",
        "            super(DAN, self).__init__()\n",
        "            self.input_size = input_size\n",
        "            self.hidden_size  = hidden_size\n",
        "            self.bn1 = nn.BatchNorm1d(input_size)\n",
        "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "            self.dropout1 = nn.Dropout(dp)\n",
        "            self.bn2 = nn.BatchNorm1d(self.hidden_size)\n",
        "            self.fc2 = torch.nn.Linear(self.hidden_size, 10)\n",
        "            self.fc3 = torch.nn.Linear(10, d_out)         \n",
        "            \n",
        "        def forward(self, x):\n",
        "            # x = self.dropout1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.fc1(x)\n",
        "            x = self.dropout1(x)\n",
        "            x = self.bn2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.fc3(x)\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPqDWAgIjIx_"
      },
      "source": [
        "#### Model, Criterion, and Optimizer\n",
        "\n",
        "**Cross Entropy Loss** : Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
        "\n",
        "**Adam Optimizer** : Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3v1fAdOjPBP"
      },
      "source": [
        "# Dimension as 50 and layers as 32 \n",
        "model = DAN(50, 25)  \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# We will set the learning rate (lr) as 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgs9twaEJfXw"
      },
      "source": [
        "Let us have a glance at the structure of model that we have built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTaHNdN55lZY"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqrWwB87jy3q"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyrMcQ5mkHkw"
      },
      "source": [
        "# First switch the module mode to model.train() so that new weights can be learned after every epoch. \n",
        "model.train()\n",
        "\n",
        "# No of Epochs\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Iterate through all the batches in each epoch\n",
        "  for inputs, target in train_loader:\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute Loss\n",
        "    target = target.squeeze_()\n",
        "    target = target.type(torch.LongTensor)\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "   \n",
        "  print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "  \n",
        "  # optimizer.step() updates the weights accordingly\n",
        "  optimizer.step()\n",
        "\n",
        "print(\"We got the training loss as %f for %d epochs\" %((loss.item(), epochs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLlObe69kWZz"
      },
      "source": [
        "#### Model evaluation for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iywetRZy51V2"
      },
      "source": [
        "# Creating empty lists to store the labels and the predictions\n",
        "labels = []\n",
        "predictions = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky3PEr-ZkbRj"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "for inputs,target in test_loader:\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    _,out = torch.max(outputs, 1)\n",
        "\n",
        "    labels.append(out)\n",
        "\n",
        "    target = target.squeeze_()\n",
        "    target = target.type(torch.LongTensor)\n",
        "\n",
        "    predictions.append(target)\n",
        "    loss = criterion(outputs,target)\n",
        "\n",
        "print(\"We got the loss as %f for test set.\" %((loss.item())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYb2YnGORmYf"
      },
      "source": [
        "Further, let us check how accurate is the model by checking how many labels matches the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNk23jUA72So"
      },
      "source": [
        "labels = torch.cat(labels, 0)\n",
        "predictions = torch.cat(predictions,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7b71Y2h-L2y"
      },
      "source": [
        "j = 0\n",
        "for i in range(662):\n",
        "  if labels[i] == predictions[i]:\n",
        "    j+=1\n",
        "print(\"%d predicted values matches the label out of total %d training set values.\" %((j,len(test_dataset))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mXnAG_rcNn1"
      },
      "source": [
        "**Statement** : Deep unordered model that obtains near state of art accuracy on sentence and document level tasks with very less training time works in the following steps:\n",
        "\n",
        "(a) take the vector average of the embeddings associated with an input sequence of tokens\n",
        "\n",
        "(b) pass that average through one or more feed-forward layer\n",
        "\n",
        "(c) perform (linear) classification on the final layers representation\n",
        "\n",
        "(d) Mean squared error.\n",
        "\n",
        "(e) Loss function is cross entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Z-J1LJDzoisp"
      },
      "source": [
        "#@title Q.1. Which of the following steps are true for the above statement? \n",
        "Answer1 = \"\" #@param [\"\",\"a, b, and c\",\"a and b\",\"a, b, c, and d\",\"a, b, c, and e\",\"All of the above\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EBa9MGLIgN4"
      },
      "source": [
        "**Consider the following statements for GloVe and answer Question 2**\n",
        "\n",
        "A. GloVe creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words. \n",
        "\n",
        "B.  GloVe embeddings are learnt based on matrix factorization techniques.\n",
        "\n",
        "C.  GloVe does not use neural networks.\n",
        "\n",
        "D.   GloVe embeddings are based on training a shallow feedforward neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu3sufDD7pu1",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Which of the above statement(s) is TRUE for GloVe?\n",
        "Answer2 = \"\" #@param [\"\",\"A and B\",\"B and C\",\"A, B, and C \",\"A and D\",\"only D\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}